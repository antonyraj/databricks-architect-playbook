# Lakeflow CDC & SCD Demo

This repository demonstrates a Spark Lakeflow Declarative Pipeline using:
- Streaming ingestion (Auto Loader)
- Data quality expectations
- CDC (SCD Type 1)
- Slowly Changing Dimension Type 2
- Gold analytical table

## Architecture
Bronze â†’ Silver â†’ CDC â†’ SCD2 â†’ Gold

## How to Deploy

```bash
databricks bundle validate
databricks bundle deploy


Then start the pipeline from the Databricks UI.

Key Concepts

Declarative pipelines (no write logic)

Automatic DAG inference

Built-in data quality & lineage

Managed state and recovery


---

# â–¶ï¸ How to Run (Step-by-Step)

```bash
git clone https://github.com/you/lakeflow-cdc-scd-demo.git
cd lakeflow-cdc-scd-demo

databricks auth login
databricks bundle deploy


Then:

Go to Pipelines

Start customer-lakeflow-cdc-scd



Azure SQL Database
      â†“ (JDBC / CDC)
Lakeflow Bronze (raw)
      â†“
Silver (clean + validated)
      â†“
CDC / SCD Type 1 & 2
      â†“
Gold (analytics)

## Data Source Variants

- `customer_pipeline_sql.py`  
  Reads customer data from Azure SQL Database using JDBC and applies CDC + SCD logic.

- `customer_pipeline_files.py`  
  File-based ingestion using Auto Loader (demo / local testing).


ğŸ§  Mental Model (Very Important)
Concern	      Where it lives
Source-specific   logic	src/pipelines/*.py
Pipeline          definition	resources/*.yml
Environment       config	databricks.yml
Secrets	      Databricks Secret Scope

File	                        Purpose
customer_pipeline_files.py	Demo / local / quick testing
customer_pipeline_sql.py	Real Azure SQL source

lakeflow-cdc-scd-demo/
â”‚
â”œâ”€â”€ databricks.yml
â”‚
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ lakeflow_pipeline.yml   ğŸ‘ˆ switch source here
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ customer_pipeline_files.py   ğŸ‘ˆ file-based (hard-coded/demo)
â”‚       â””â”€â”€ customer_pipeline_sql.py     ğŸ‘ˆ Azure SQL-based (real)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/
â”‚       â””â”€â”€ customers.json
â”‚
â””â”€â”€ README.md

