# Databricks Architect Execution Tracker

This tracker is used to validate hands-on progress toward **Databricks Delivery Architect readiness**.
Check items only after hands-on completion.

---

## Week 1 – Spark & Lakehouse Foundations
- [ ] Spark architecture understood (Driver, Executors, DAG, shuffle)
- [ ] SQL → PySpark transformations implemented
- [ ] Delta table created with history
- [ ] Time travel & rollback tested
- [ ] Bronze → Silver → Gold pipeline built

**Notes / Gaps:**

---

## Week 2 – Greenfield (Azure, AWS, GCP)
- [ ] Customer onboarding checklist completed
- [ ] Azure Databricks architecture documented
- [ ] AWS Databricks architecture documented
- [ ] GCP Databricks architecture documented
- [ ] Storage + identity integration validated
- [ ] Dev/Test/Prod strategy defined

**Notes / Gaps:**

---

## Week 3 – Ingestion & Processing Patterns
- [ ] Batch ingestion using Auto Loader
- [ ] Streaming pipeline implemented
- [ ] CDC implemented using MERGE INTO
- [ ] Databricks Workflow created
- [ ] Tooling decision document completed

**Notes / Gaps:**

---

## Week 4 – Migration Mastery
- [ ] Universal migration framework documented
- [ ] Oracle / SQL Server migration completed
- [ ] Hadoop / Hive migration completed
- [ ] Snowflake migration completed
- [ ] Parallel run & reconciliation tested

**Notes / Gaps:**

---

## Week 5 – Costing & Proposals
- [ ] DBU cost model understood
- [ ] Cluster sizing matrix created
- [ ] Proposal template completed
- [ ] Cost optimization scenarios documented

**Notes / Gaps:**

---

## Week 6 – Security & Governance
- [ ] Unity Catalog configured
- [ ] RBAC implemented
- [ ] Column masking & row filters applied
- [ ] Audit logs & lineage reviewed
- [ ] Catalog & schema blueprint finalized

**Notes / Gaps:**

---

## Final Verdict
- [ ] All sections completed
- [ ] Ready to operate as Hands-on Databricks Delivery Architect
